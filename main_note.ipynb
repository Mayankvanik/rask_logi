{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index has been saved to disk.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "doc_embeddings = [embedding_model.embed_documents([chunk.page_content]) for chunk in doc_chunks]\n",
    "\n",
    "\n",
    "faiss_store = FAISS.from_texts([chunk.page_content for chunk in doc_chunks], embedding_model)\n",
    "\n",
    "\n",
    "faiss_store.save_local(\"C:/python/task01/main_env/faiss_index\")\n",
    "\n",
    "print(\"FAISS index has been saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(groq_api_key=api, model_name='llama3-8b-8192')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog series GenAI | A brief introduction in Generative AI | by JoinSeven | MediumOpen in appSign upSign inWriteSign upSign inBlog series GenAI | A brief introduction in Generative AIJoinSeven·Follow7 min read·Nov 2, 2023--2ListenShareIt has escaped almost no one’s attention: the rapid developments surrounding Generative AI (GenAI). Generative AI has skyrocketed in popularity in 2023, thanks in large part to breakthroughs such as OpenAI’s ChatGPT and DALL-E. It is estimated that more than one and a half million Dutch people are currently working with the technology, for example to compose emails, generate images or experiment with new applications themselves.This blog is an English translation of the original blog on our website.An image representing the topic of this blog series, created with Dall-E.Technology offers opportunities to perform work better and more efficiently. According to recent research by PWC, more than 44% of Dutch jobs are highly “exposed” to GenAI and the technology could be a solution to the persistent labor market shortage in the Netherlands. A huge shift is currently taking place in education, as students are now receiving massive support from ChatGPT in completing assignments and writing essays and theses. Universities are looking for ways to make technology part of education in a constructive way.In our company we also notice a growing interest in GenAI. From the boardroom to the coffee corner, applications are currently being discussed in almost every company. And every day there is a knowledge meeting or a webinar on the subject somewhere.Generative AI is not yet a success story for everyoneAs with all news, you mainly read the enormous success stories (or, on the contrary, risks and doom stories). In practice, many organizations in the Netherlands have not changed much and the structural embedding of the value of GenAI often lags behind. Some of the questions we are frequently asked by individuals and organizations are:“What is Generatieve AI?”“What can AI do for my organization?”“How do I start a successful AI-project?”“What does it take to implement AI?”“How do I implement AI in a responsible way?”Blog series GenerativeThat’s why we’re launching this blog series about Generative AI. By sharing our knowledge, we want to help organizations take the step from “talking and considering” to realizing successful and responsible AI within their organization. Until the end of this year, we will highlight one topic for a successful AI implementation every week. We discuss topics such as finding a suitable use case, establishing a solid technical foundation, selecting and training your language model, using AI responsibly and finally validating and implementing your AI solution. We mainly focus on AI solutions aimed at textual content.In this first blog we discuss the Generative AI concept in broad terms: We discuss what it is, how it works and what opportunities and challenges there are when implementing it in your organization. We’ll also be previewing the next blogs in this series.Generative AI: What is it?Let’s start with the concept of Generative AI. This is more than a fashionable buzzword. It is a form of artificial intelligence in which algorithms generate data instead of just analyzing it. GenAI makes it possible to create new content, ranging from text to images, videos, speech and even music. It can thus achieve realistic and personal interactions between humans and machines.Several specific examples of where Generative AI is used for:Answering first-line customer questions about the services provided by organizations.Writing promotional and marketing materials by marketers and content writers.Generating images based on textual descriptions (or vice versa).Writing, checking and improving programming code or queries by developers.Take for example a Generative AI in the context of a service provider like us. Based on data about frequently asked questions and customer reviews, an AI can generate personalized and relevant answers to a new customer question. For a new question, such as about our opening hours or how an appointment can be scheduled, the AI ​​first interprets the assignment/question, retrieves the necessary information from our (internal and public) data and then formulates a unique and personalized answer to the customer.This is a fairly simple example. But you can also apply similar interactions in other, more complex business situations. In the policy context, you can think of an application for answering parliamentary questions, for answering Information Notes when carrying out European tenders or for recognizing commercial opportunities for companies based on market information. If you want to know more about these applications, take a look at our portfolio of AI projects.The engine behind Generative AI: Large Language Models (LLMs)Generative AI models learn from large amounts of data. They analyze this data to recognize patterns and relationships. The engine behind this is a language model (also called “Large Language Model” or “LMM”). Consider a language model as an algorithm that learns to understand the structure and use of language. Language models are based on statistical analyzes of texts. They calculate the probability of word orders in sentences.Modern language models, such as those based on Transformer architectures (e.g. Open AI’s GPT-3 and 4, Google Bard and Meta’s LLaMa), use multi-layered neural networks to understand relationships between words and phrases in a text. Texts are first divided into pieces (“tokens”), which can vary from one character to whole words. These tokens are converted into numerical vectors (“embeddings”) that capture the meaning and relationships between words.First, the model is trained on a general dataset and then it can be tuned for specific tasks on a specific dataset for the relevant use case, such as answering customer questions or generating texts, images or music in a certain context or genre.Success factors in AI-implementationsThrough our experience in the field of AI and innovative data projects, we have gained a clear picture of what is needed to implement Generative AI successfully and responsibly. Some of these aspects are specifically important for projects in the field of GenAI, but many are also broadly applicable to projects with a major technological and innovative character.When implementing Generative AI in your organization, consider the following elements:You need a clear use case. Our advice is: Find a solution to a problem and not the other way around! First make an inventory of problems you want to solve, and then create a vision of how (Generative) AI can solve this problem.Successful use of AI requires a solid information strategy. This also applies to AI; “garbage in, garbage out”. This is not just about the language models you use, but also the data and systems that feed these models.Train and tune your AI model for your specific use case. Most use cases require more than an “off-the-shelf” solution. Training and/or tuning your language model for your use case makes a big difference.Take into account ethical and social considerations, such as privacy and algorithm bias. Technology in itself is not “good” or “bad”, but incorrect implementation can cause unnecessary risks. We stand for responsible and human-oriented use of AI.Choose an innovation-oriented, people-oriented and short-cycle approach. A Generative AI project is innovative by definition. Validate assumptions and investigate unknown facets of AI in your organization. Our Data Discovery Sprint implements this in a concrete, user-oriented and fast way.Guide people in your organization to work with AI. Generative AI turns business processes upside down and many people find it exciting. A successful implementation therefore requires a thorough implementation strategy with an eye for the people who (will) work with it, the technology and work processes. This is the only way to ensure that an AI project has a truly positive impact.ConclusionIn this blog we gave a brief introduction to our blog series about GenAI. We gave an introduction to what Generative AI is, what you can apply it to and how it works in general terms. Finally, we presented a number of success factors for AI projects based on our vision. We will discuss all of these factors in a separate blog in this series.A brief summary of key-points in this blog:GenAI is turning the world upside down and is more than a fashionable “buzzword”.GenAI works on the basis of a language model that predicts word orders based on Deep Learning.This language model does not contain all the “knowledge” itself, but bases it on the data on which it was trained.Success factors such as a suitable use case, technological foundation and innovative methods are crucial for an impactful AI implementation.OutlookIn the following blogs we will take you through all the steps and considerations for a successful implementation of GenAI in your organization. We provide you with the necessary knowledge and tools to set up a successful AI project in your organization. So keep following us and register for the next blogs or the Whitepaper via the form on our website!In the next weeks, we will publish the following blogs:Blog 1: Introduction in Generative AIBlog 2: Select your use caseBlog 3: A solid ground for you AI-projectBlog 4: Select, train and tune your AI-modelBlog 5: Ethical considerations in AI-projectsBlog 6: Validate your AI-solutionBlog 7: Guide your organization in working with AIBlog 8: Recap — Lessons learned from this blog seriesArtificial IntelligenceNaturallanguageprocessingAIMachine LearningInnovation----2FollowWritten by JoinSeven13 FollowersWe increase the impact of organizations through the development of intelligent apps with AI, dashboards and other data-driven applications.FollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\task01\\main_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index has been saved to disk.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "url = \"https://joinseven.medium.com/blog-series-genai-a-brief-introduction-in-generative-ai-4e11154df3f2\"\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "#docs[0]\n",
    "\n",
    "def clean_page_content(text):\n",
    "    cleaned_text = text.replace('\\xa0', ' ')\n",
    "    cleaned_text = re.sub(r'\\x0c', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\.{2,}', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Apply the cleaning function to each document's content\n",
    "for i, doc in enumerate(docs):\n",
    "    cleaned_content = clean_page_content(doc.page_content)  # Apply cleaning\n",
    "    docs[i].page_content = cleaned_content  # Save cleaned content back to the document\n",
    "\n",
    "# Example of printing out the cleaned document\n",
    "print(docs[0].page_content)\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  \n",
    "    chunk_overlap=20  \n",
    ")\n",
    "\n",
    "\n",
    "doc_chunks = text_splitter.split_documents(docs)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "doc_embeddings = [embedding_model.embed_documents([chunk.page_content]) for chunk in doc_chunks]\n",
    "faiss_store = FAISS.from_texts([chunk.page_content for chunk in doc_chunks], embedding_model)\n",
    "faiss_store.save_local(\"C:/python/task01/main_env/faiss_index\")\n",
    "\n",
    "print(\"FAISS index has been saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sir, How Can I Help You!\n",
      "\n",
      "According to the context provided, GenAI stands for Generative AI, which has gained popularity in 2023 due to breakthroughs such as OpenAI's ChatGPT and DALL-E.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load the FAISS \n",
    "faiss_store = FAISS.load_local(\"C:/python/task01/main_env/faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "question = \"what is GenAI\"\n",
    "\n",
    "#perform similarity\n",
    "results = faiss_store.similarity_search(question, k=3)\n",
    "\n",
    "llm = ChatGroq(groq_api_key=api, model_name='llama3-8b-8192')\n",
    "\n",
    "#prompt template\n",
    "prompt_template = \"\"\"If User Greet then Hello Sir, How Can I Help You! Else Answer the question asked by user. You have to help them and understand information related to question as much as possible using the provided context. If the answer is not contained in the context, say \"Please Ask Full Question About Policy and Rules Related\" \\n\\n\n",
    "Context: \\n {context}?\\n\n",
    "Question: \\n {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Load the QA chain\n",
    "stuff_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "# Run the chain and get the answer\n",
    "stuff_answer = stuff_chain(\n",
    "    {\"input_documents\": results, \"question\": question}, return_only_outputs=True)\n",
    "\n",
    "# Print the answer\n",
    "print(stuff_answer['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Sir, How Can I Help You!\n",
      "\n",
      "Residual Networks, or ResNets, are a type of deep neural network architecture that offers significant improvements in training deep neural networks while mitigating the vanishing gradient problem.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is Residual Networks\"\n",
    "\n",
    "faiss_store = FAISS.load_local(\"C:/python/task01/main_env/faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "#perform similarity\n",
    "results = faiss_store.similarity_search(question, k=3)\n",
    "\n",
    "\n",
    "# Run the chain and get the answer\n",
    "stuff_answer = stuff_chain(\n",
    "    {\"input_documents\": results, \"question\": question}, return_only_outputs=True)\n",
    "\n",
    "# Print the answer\n",
    "print(stuff_answer['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
